[2025-01-06T20:56:42.991+0000] {processor.py:161} INFO - Started process (PID=4016) to work on /opt/airflow/dags/api-timelapse.py
[2025-01-06T20:56:42.992+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/api-timelapse.py for tasks to queue
[2025-01-06T20:56:42.995+0000] {logging_mixin.py:188} INFO - [2025-01-06T20:56:42.994+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/api-timelapse.py
[2025-01-06T20:56:43.045+0000] {processor.py:840} INFO - DAG(s) 'python-timelapse-workflow' retrieved from /opt/airflow/dags/api-timelapse.py
[2025-01-06T20:56:43.210+0000] {logging_mixin.py:188} INFO - [2025-01-06T20:56:43.210+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-01-06T20:56:43.230+0000] {logging_mixin.py:188} INFO - [2025-01-06T20:56:43.230+0000] {dag.py:3823} INFO - Setting next_dagrun for python-timelapse-workflow to None, run_after=None
[2025-01-06T20:56:43.262+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/api-timelapse.py took 0.279 seconds
[2025-01-06T20:57:13.511+0000] {processor.py:161} INFO - Started process (PID=4028) to work on /opt/airflow/dags/api-timelapse.py
[2025-01-06T20:57:13.514+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/api-timelapse.py for tasks to queue
[2025-01-06T20:57:13.521+0000] {logging_mixin.py:188} INFO - [2025-01-06T20:57:13.520+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/api-timelapse.py
[2025-01-06T20:57:13.570+0000] {processor.py:840} INFO - DAG(s) 'python-timelapse-workflow' retrieved from /opt/airflow/dags/api-timelapse.py
[2025-01-06T20:57:13.793+0000] {logging_mixin.py:188} INFO - [2025-01-06T20:57:13.792+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-01-06T20:57:13.815+0000] {logging_mixin.py:188} INFO - [2025-01-06T20:57:13.815+0000] {dag.py:3823} INFO - Setting next_dagrun for python-timelapse-workflow to None, run_after=None
[2025-01-06T20:57:13.861+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/api-timelapse.py took 0.356 seconds
[2025-01-06T20:57:44.031+0000] {processor.py:161} INFO - Started process (PID=4040) to work on /opt/airflow/dags/api-timelapse.py
[2025-01-06T20:57:44.033+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/api-timelapse.py for tasks to queue
[2025-01-06T20:57:44.036+0000] {logging_mixin.py:188} INFO - [2025-01-06T20:57:44.036+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/api-timelapse.py
[2025-01-06T20:57:44.089+0000] {processor.py:840} INFO - DAG(s) 'python-timelapse-workflow' retrieved from /opt/airflow/dags/api-timelapse.py
[2025-01-06T20:57:44.258+0000] {logging_mixin.py:188} INFO - [2025-01-06T20:57:44.257+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-01-06T20:57:44.277+0000] {logging_mixin.py:188} INFO - [2025-01-06T20:57:44.276+0000] {dag.py:3823} INFO - Setting next_dagrun for python-timelapse-workflow to None, run_after=None
[2025-01-06T20:57:44.512+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/api-timelapse.py took 0.487 seconds
[2025-01-06T20:58:14.764+0000] {processor.py:161} INFO - Started process (PID=4052) to work on /opt/airflow/dags/api-timelapse.py
[2025-01-06T20:58:14.766+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/api-timelapse.py for tasks to queue
[2025-01-06T20:58:14.769+0000] {logging_mixin.py:188} INFO - [2025-01-06T20:58:14.768+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/api-timelapse.py
[2025-01-06T20:58:14.813+0000] {processor.py:840} INFO - DAG(s) 'python-timelapse-workflow' retrieved from /opt/airflow/dags/api-timelapse.py
[2025-01-06T20:58:14.976+0000] {logging_mixin.py:188} INFO - [2025-01-06T20:58:14.976+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-01-06T20:58:15.201+0000] {logging_mixin.py:188} INFO - [2025-01-06T20:58:15.200+0000] {dag.py:3823} INFO - Setting next_dagrun for python-timelapse-workflow to None, run_after=None
[2025-01-06T20:58:15.223+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/api-timelapse.py took 0.467 seconds
[2025-01-06T20:58:45.729+0000] {processor.py:161} INFO - Started process (PID=4064) to work on /opt/airflow/dags/api-timelapse.py
[2025-01-06T20:58:45.730+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/api-timelapse.py for tasks to queue
[2025-01-06T20:58:45.733+0000] {logging_mixin.py:188} INFO - [2025-01-06T20:58:45.733+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/api-timelapse.py
[2025-01-06T20:58:45.777+0000] {processor.py:840} INFO - DAG(s) 'python-timelapse-workflow' retrieved from /opt/airflow/dags/api-timelapse.py
[2025-01-06T20:58:45.982+0000] {logging_mixin.py:188} INFO - [2025-01-06T20:58:45.981+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-01-06T20:58:45.997+0000] {logging_mixin.py:188} INFO - [2025-01-06T20:58:45.997+0000] {dag.py:3823} INFO - Setting next_dagrun for python-timelapse-workflow to None, run_after=None
[2025-01-06T20:58:46.031+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/api-timelapse.py took 0.308 seconds
[2025-01-06T20:59:16.423+0000] {processor.py:161} INFO - Started process (PID=4068) to work on /opt/airflow/dags/api-timelapse.py
[2025-01-06T20:59:16.425+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/api-timelapse.py for tasks to queue
[2025-01-06T20:59:16.428+0000] {logging_mixin.py:188} INFO - [2025-01-06T20:59:16.428+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/api-timelapse.py
[2025-01-06T20:59:16.477+0000] {processor.py:840} INFO - DAG(s) 'python-timelapse-workflow' retrieved from /opt/airflow/dags/api-timelapse.py
[2025-01-06T20:59:16.646+0000] {logging_mixin.py:188} INFO - [2025-01-06T20:59:16.645+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-01-06T20:59:16.663+0000] {logging_mixin.py:188} INFO - [2025-01-06T20:59:16.662+0000] {dag.py:3823} INFO - Setting next_dagrun for python-timelapse-workflow to None, run_after=None
[2025-01-06T20:59:16.699+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/api-timelapse.py took 0.282 seconds
[2025-01-06T20:59:47.201+0000] {processor.py:161} INFO - Started process (PID=4080) to work on /opt/airflow/dags/api-timelapse.py
[2025-01-06T20:59:47.202+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/api-timelapse.py for tasks to queue
[2025-01-06T20:59:47.205+0000] {logging_mixin.py:188} INFO - [2025-01-06T20:59:47.204+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/api-timelapse.py
[2025-01-06T20:59:47.257+0000] {processor.py:840} INFO - DAG(s) 'python-timelapse-workflow' retrieved from /opt/airflow/dags/api-timelapse.py
[2025-01-06T20:59:47.437+0000] {logging_mixin.py:188} INFO - [2025-01-06T20:59:47.436+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-01-06T20:59:47.458+0000] {logging_mixin.py:188} INFO - [2025-01-06T20:59:47.458+0000] {dag.py:3823} INFO - Setting next_dagrun for python-timelapse-workflow to None, run_after=None
[2025-01-06T20:59:47.493+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/api-timelapse.py took 0.299 seconds
[2025-01-06T21:00:17.693+0000] {processor.py:161} INFO - Started process (PID=4092) to work on /opt/airflow/dags/api-timelapse.py
[2025-01-06T21:00:17.695+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/api-timelapse.py for tasks to queue
[2025-01-06T21:00:17.698+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:00:17.698+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/api-timelapse.py
[2025-01-06T21:00:17.770+0000] {processor.py:840} INFO - DAG(s) 'python-timelapse-workflow' retrieved from /opt/airflow/dags/api-timelapse.py
[2025-01-06T21:00:18.024+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:00:18.024+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-01-06T21:00:18.050+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:00:18.049+0000] {dag.py:3823} INFO - Setting next_dagrun for python-timelapse-workflow to None, run_after=None
[2025-01-06T21:00:18.326+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/api-timelapse.py took 0.643 seconds
[2025-01-06T21:00:48.538+0000] {processor.py:161} INFO - Started process (PID=4104) to work on /opt/airflow/dags/api-timelapse.py
[2025-01-06T21:00:48.540+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/api-timelapse.py for tasks to queue
[2025-01-06T21:00:48.544+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:00:48.544+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/api-timelapse.py
[2025-01-06T21:00:48.603+0000] {processor.py:840} INFO - DAG(s) 'python-timelapse-workflow' retrieved from /opt/airflow/dags/api-timelapse.py
[2025-01-06T21:00:48.814+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:00:48.813+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-01-06T21:00:49.034+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:00:49.034+0000] {dag.py:3823} INFO - Setting next_dagrun for python-timelapse-workflow to None, run_after=None
[2025-01-06T21:00:49.061+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/api-timelapse.py took 0.529 seconds
[2025-01-06T21:01:19.549+0000] {processor.py:161} INFO - Started process (PID=4116) to work on /opt/airflow/dags/api-timelapse.py
[2025-01-06T21:01:19.552+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/api-timelapse.py for tasks to queue
[2025-01-06T21:01:19.556+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:01:19.555+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/api-timelapse.py
[2025-01-06T21:01:19.623+0000] {processor.py:840} INFO - DAG(s) 'python-timelapse-workflow' retrieved from /opt/airflow/dags/api-timelapse.py
[2025-01-06T21:01:19.826+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:01:19.826+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-01-06T21:01:19.848+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:01:19.847+0000] {dag.py:3823} INFO - Setting next_dagrun for python-timelapse-workflow to None, run_after=None
[2025-01-06T21:01:19.889+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/api-timelapse.py took 0.345 seconds
[2025-01-06T21:01:50.425+0000] {processor.py:161} INFO - Started process (PID=4128) to work on /opt/airflow/dags/api-timelapse.py
[2025-01-06T21:01:50.427+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/api-timelapse.py for tasks to queue
[2025-01-06T21:01:50.430+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:01:50.429+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/api-timelapse.py
[2025-01-06T21:01:50.481+0000] {processor.py:840} INFO - DAG(s) 'python-timelapse-workflow' retrieved from /opt/airflow/dags/api-timelapse.py
[2025-01-06T21:01:50.654+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:01:50.653+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-01-06T21:01:50.672+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:01:50.672+0000] {dag.py:3823} INFO - Setting next_dagrun for python-timelapse-workflow to None, run_after=None
[2025-01-06T21:01:50.708+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/api-timelapse.py took 0.288 seconds
[2025-01-06T21:02:21.312+0000] {processor.py:161} INFO - Started process (PID=4141) to work on /opt/airflow/dags/api-timelapse.py
[2025-01-06T21:02:21.314+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/api-timelapse.py for tasks to queue
[2025-01-06T21:02:21.316+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:02:21.316+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/api-timelapse.py
[2025-01-06T21:02:21.362+0000] {processor.py:840} INFO - DAG(s) 'python-timelapse-workflow' retrieved from /opt/airflow/dags/api-timelapse.py
[2025-01-06T21:02:21.537+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:02:21.537+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-01-06T21:02:21.554+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:02:21.553+0000] {dag.py:3823} INFO - Setting next_dagrun for python-timelapse-workflow to None, run_after=None
[2025-01-06T21:02:21.581+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/api-timelapse.py took 0.276 seconds
[2025-01-06T21:02:51.886+0000] {processor.py:161} INFO - Started process (PID=4155) to work on /opt/airflow/dags/api-timelapse.py
[2025-01-06T21:02:51.888+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/api-timelapse.py for tasks to queue
[2025-01-06T21:02:51.893+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:02:51.892+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/api-timelapse.py
[2025-01-06T21:02:51.938+0000] {processor.py:840} INFO - DAG(s) 'python-timelapse-workflow' retrieved from /opt/airflow/dags/api-timelapse.py
[2025-01-06T21:02:52.105+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:02:52.104+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-01-06T21:02:52.127+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:02:52.126+0000] {dag.py:3823} INFO - Setting next_dagrun for python-timelapse-workflow to None, run_after=None
[2025-01-06T21:02:52.346+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/api-timelapse.py took 0.465 seconds
[2025-01-06T21:03:22.916+0000] {processor.py:161} INFO - Started process (PID=4168) to work on /opt/airflow/dags/api-timelapse.py
[2025-01-06T21:03:22.918+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/api-timelapse.py for tasks to queue
[2025-01-06T21:03:22.919+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:03:22.919+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/api-timelapse.py
[2025-01-06T21:03:22.969+0000] {processor.py:840} INFO - DAG(s) 'python-timelapse-workflow' retrieved from /opt/airflow/dags/api-timelapse.py
[2025-01-06T21:03:23.183+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:03:23.183+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-01-06T21:03:23.413+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:03:23.412+0000] {dag.py:3823} INFO - Setting next_dagrun for python-timelapse-workflow to None, run_after=None
[2025-01-06T21:03:23.446+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/api-timelapse.py took 0.536 seconds
[2025-01-06T21:03:53.938+0000] {processor.py:161} INFO - Started process (PID=4180) to work on /opt/airflow/dags/api-timelapse.py
[2025-01-06T21:03:53.941+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/api-timelapse.py for tasks to queue
[2025-01-06T21:03:53.943+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:03:53.942+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/api-timelapse.py
[2025-01-06T21:03:54.003+0000] {processor.py:840} INFO - DAG(s) 'python-timelapse-workflow' retrieved from /opt/airflow/dags/api-timelapse.py
[2025-01-06T21:03:54.194+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:03:54.193+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-01-06T21:03:54.212+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:03:54.211+0000] {dag.py:3823} INFO - Setting next_dagrun for python-timelapse-workflow to None, run_after=None
[2025-01-06T21:03:54.242+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/api-timelapse.py took 0.311 seconds
[2025-01-06T21:04:24.746+0000] {processor.py:161} INFO - Started process (PID=4184) to work on /opt/airflow/dags/api-timelapse.py
[2025-01-06T21:04:24.748+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/api-timelapse.py for tasks to queue
[2025-01-06T21:04:24.749+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:04:24.749+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/api-timelapse.py
[2025-01-06T21:04:24.797+0000] {processor.py:840} INFO - DAG(s) 'python-timelapse-workflow' retrieved from /opt/airflow/dags/api-timelapse.py
[2025-01-06T21:04:24.970+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:04:24.970+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-01-06T21:04:24.987+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:04:24.986+0000] {dag.py:3823} INFO - Setting next_dagrun for python-timelapse-workflow to None, run_after=None
[2025-01-06T21:04:25.013+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/api-timelapse.py took 0.274 seconds
[2025-01-06T21:04:55.507+0000] {processor.py:161} INFO - Started process (PID=4196) to work on /opt/airflow/dags/api-timelapse.py
[2025-01-06T21:04:55.508+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/api-timelapse.py for tasks to queue
[2025-01-06T21:04:55.510+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:04:55.509+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/api-timelapse.py
[2025-01-06T21:04:55.565+0000] {processor.py:840} INFO - DAG(s) 'python-timelapse-workflow' retrieved from /opt/airflow/dags/api-timelapse.py
[2025-01-06T21:04:56.117+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (192.168.48.3), port 5432 failed: FATAL:  could not open file "global/pg_filenode.map": No such file or directory


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (192.168.48.3), port 5432 failed: FATAL:  could not open file "global/pg_filenode.map": No such file or directory

(Background on this error at: https://sqlalche.me/e/14/e3q8)
