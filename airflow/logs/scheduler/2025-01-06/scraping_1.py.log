[2025-01-06T20:56:42.996+0000] {processor.py:161} INFO - Started process (PID=4017) to work on /opt/airflow/dags/scraping_1.py
[2025-01-06T20:56:42.997+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/scraping_1.py for tasks to queue
[2025-01-06T20:56:43.001+0000] {logging_mixin.py:188} INFO - [2025-01-06T20:56:43.000+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/scraping_1.py
[2025-01-06T20:56:43.045+0000] {processor.py:840} INFO - DAG(s) 'filtered_links_workflow' retrieved from /opt/airflow/dags/scraping_1.py
[2025-01-06T20:56:43.208+0000] {logging_mixin.py:188} INFO - [2025-01-06T20:56:43.208+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-01-06T20:56:43.224+0000] {logging_mixin.py:188} INFO - [2025-01-06T20:56:43.223+0000] {dag.py:3823} INFO - Setting next_dagrun for filtered_links_workflow to None, run_after=None
[2025-01-06T20:56:43.255+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/scraping_1.py took 0.264 seconds
[2025-01-06T20:57:13.527+0000] {processor.py:161} INFO - Started process (PID=4029) to work on /opt/airflow/dags/scraping_1.py
[2025-01-06T20:57:13.528+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/scraping_1.py for tasks to queue
[2025-01-06T20:57:13.531+0000] {logging_mixin.py:188} INFO - [2025-01-06T20:57:13.531+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/scraping_1.py
[2025-01-06T20:57:13.580+0000] {processor.py:840} INFO - DAG(s) 'filtered_links_workflow' retrieved from /opt/airflow/dags/scraping_1.py
[2025-01-06T20:57:13.806+0000] {logging_mixin.py:188} INFO - [2025-01-06T20:57:13.805+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-01-06T20:57:13.827+0000] {logging_mixin.py:188} INFO - [2025-01-06T20:57:13.827+0000] {dag.py:3823} INFO - Setting next_dagrun for filtered_links_workflow to None, run_after=None
[2025-01-06T20:57:13.862+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/scraping_1.py took 0.342 seconds
[2025-01-06T20:57:44.038+0000] {processor.py:161} INFO - Started process (PID=4041) to work on /opt/airflow/dags/scraping_1.py
[2025-01-06T20:57:44.040+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/scraping_1.py for tasks to queue
[2025-01-06T20:57:44.043+0000] {logging_mixin.py:188} INFO - [2025-01-06T20:57:44.042+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/scraping_1.py
[2025-01-06T20:57:44.088+0000] {processor.py:840} INFO - DAG(s) 'filtered_links_workflow' retrieved from /opt/airflow/dags/scraping_1.py
[2025-01-06T20:57:44.263+0000] {logging_mixin.py:188} INFO - [2025-01-06T20:57:44.262+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-01-06T20:57:44.280+0000] {logging_mixin.py:188} INFO - [2025-01-06T20:57:44.280+0000] {dag.py:3823} INFO - Setting next_dagrun for filtered_links_workflow to None, run_after=None
[2025-01-06T20:57:44.517+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/scraping_1.py took 0.483 seconds
[2025-01-06T20:58:14.772+0000] {processor.py:161} INFO - Started process (PID=4053) to work on /opt/airflow/dags/scraping_1.py
[2025-01-06T20:58:14.774+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/scraping_1.py for tasks to queue
[2025-01-06T20:58:14.777+0000] {logging_mixin.py:188} INFO - [2025-01-06T20:58:14.776+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/scraping_1.py
[2025-01-06T20:58:14.825+0000] {processor.py:840} INFO - DAG(s) 'filtered_links_workflow' retrieved from /opt/airflow/dags/scraping_1.py
[2025-01-06T20:58:14.986+0000] {logging_mixin.py:188} INFO - [2025-01-06T20:58:14.986+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-01-06T20:58:15.203+0000] {logging_mixin.py:188} INFO - [2025-01-06T20:58:15.202+0000] {dag.py:3823} INFO - Setting next_dagrun for filtered_links_workflow to None, run_after=None
[2025-01-06T20:58:15.232+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/scraping_1.py took 0.466 seconds
[2025-01-06T20:58:45.740+0000] {processor.py:161} INFO - Started process (PID=4065) to work on /opt/airflow/dags/scraping_1.py
[2025-01-06T20:58:45.742+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/scraping_1.py for tasks to queue
[2025-01-06T20:58:45.744+0000] {logging_mixin.py:188} INFO - [2025-01-06T20:58:45.744+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/scraping_1.py
[2025-01-06T20:58:45.790+0000] {processor.py:840} INFO - DAG(s) 'filtered_links_workflow' retrieved from /opt/airflow/dags/scraping_1.py
[2025-01-06T20:58:45.983+0000] {logging_mixin.py:188} INFO - [2025-01-06T20:58:45.982+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-01-06T20:58:45.997+0000] {logging_mixin.py:188} INFO - [2025-01-06T20:58:45.997+0000] {dag.py:3823} INFO - Setting next_dagrun for filtered_links_workflow to None, run_after=None
[2025-01-06T20:58:46.027+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/scraping_1.py took 0.294 seconds
[2025-01-06T20:59:16.435+0000] {processor.py:161} INFO - Started process (PID=4069) to work on /opt/airflow/dags/scraping_1.py
[2025-01-06T20:59:16.437+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/scraping_1.py for tasks to queue
[2025-01-06T20:59:16.439+0000] {logging_mixin.py:188} INFO - [2025-01-06T20:59:16.439+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/scraping_1.py
[2025-01-06T20:59:16.497+0000] {processor.py:840} INFO - DAG(s) 'filtered_links_workflow' retrieved from /opt/airflow/dags/scraping_1.py
[2025-01-06T20:59:16.665+0000] {logging_mixin.py:188} INFO - [2025-01-06T20:59:16.664+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-01-06T20:59:16.689+0000] {logging_mixin.py:188} INFO - [2025-01-06T20:59:16.688+0000] {dag.py:3823} INFO - Setting next_dagrun for filtered_links_workflow to None, run_after=None
[2025-01-06T20:59:16.727+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/scraping_1.py took 0.296 seconds
[2025-01-06T20:59:47.210+0000] {processor.py:161} INFO - Started process (PID=4081) to work on /opt/airflow/dags/scraping_1.py
[2025-01-06T20:59:47.212+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/scraping_1.py for tasks to queue
[2025-01-06T20:59:47.215+0000] {logging_mixin.py:188} INFO - [2025-01-06T20:59:47.214+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/scraping_1.py
[2025-01-06T20:59:47.258+0000] {processor.py:840} INFO - DAG(s) 'filtered_links_workflow' retrieved from /opt/airflow/dags/scraping_1.py
[2025-01-06T20:59:47.434+0000] {logging_mixin.py:188} INFO - [2025-01-06T20:59:47.434+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-01-06T20:59:47.456+0000] {logging_mixin.py:188} INFO - [2025-01-06T20:59:47.455+0000] {dag.py:3823} INFO - Setting next_dagrun for filtered_links_workflow to None, run_after=None
[2025-01-06T20:59:47.490+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/scraping_1.py took 0.286 seconds
[2025-01-06T21:00:17.705+0000] {processor.py:161} INFO - Started process (PID=4093) to work on /opt/airflow/dags/scraping_1.py
[2025-01-06T21:00:17.707+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/scraping_1.py for tasks to queue
[2025-01-06T21:00:17.710+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:00:17.710+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/scraping_1.py
[2025-01-06T21:00:17.769+0000] {processor.py:840} INFO - DAG(s) 'filtered_links_workflow' retrieved from /opt/airflow/dags/scraping_1.py
[2025-01-06T21:00:18.020+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:00:18.020+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-01-06T21:00:18.044+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:00:18.044+0000] {dag.py:3823} INFO - Setting next_dagrun for filtered_links_workflow to None, run_after=None
[2025-01-06T21:00:18.317+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/scraping_1.py took 0.619 seconds
[2025-01-06T21:00:48.554+0000] {processor.py:161} INFO - Started process (PID=4105) to work on /opt/airflow/dags/scraping_1.py
[2025-01-06T21:00:48.556+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/scraping_1.py for tasks to queue
[2025-01-06T21:00:48.558+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:00:48.558+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/scraping_1.py
[2025-01-06T21:00:48.602+0000] {processor.py:840} INFO - DAG(s) 'filtered_links_workflow' retrieved from /opt/airflow/dags/scraping_1.py
[2025-01-06T21:00:48.777+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:00:48.776+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-01-06T21:00:49.001+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:00:49.000+0000] {dag.py:3823} INFO - Setting next_dagrun for filtered_links_workflow to None, run_after=None
[2025-01-06T21:00:49.033+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/scraping_1.py took 0.484 seconds
[2025-01-06T21:01:19.559+0000] {processor.py:161} INFO - Started process (PID=4117) to work on /opt/airflow/dags/scraping_1.py
[2025-01-06T21:01:19.561+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/scraping_1.py for tasks to queue
[2025-01-06T21:01:19.563+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:01:19.563+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/scraping_1.py
[2025-01-06T21:01:19.612+0000] {processor.py:840} INFO - DAG(s) 'filtered_links_workflow' retrieved from /opt/airflow/dags/scraping_1.py
[2025-01-06T21:01:19.817+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:01:19.816+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-01-06T21:01:19.835+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:01:19.834+0000] {dag.py:3823} INFO - Setting next_dagrun for filtered_links_workflow to None, run_after=None
[2025-01-06T21:01:19.863+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/scraping_1.py took 0.312 seconds
[2025-01-06T21:01:50.446+0000] {processor.py:161} INFO - Started process (PID=4129) to work on /opt/airflow/dags/scraping_1.py
[2025-01-06T21:01:50.448+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/scraping_1.py for tasks to queue
[2025-01-06T21:01:50.450+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:01:50.450+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/scraping_1.py
[2025-01-06T21:01:50.497+0000] {processor.py:840} INFO - DAG(s) 'filtered_links_workflow' retrieved from /opt/airflow/dags/scraping_1.py
[2025-01-06T21:01:50.737+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:01:50.736+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-01-06T21:01:50.764+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:01:50.763+0000] {dag.py:3823} INFO - Setting next_dagrun for filtered_links_workflow to None, run_after=None
[2025-01-06T21:01:50.800+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/scraping_1.py took 0.359 seconds
[2025-01-06T21:02:21.321+0000] {processor.py:161} INFO - Started process (PID=4142) to work on /opt/airflow/dags/scraping_1.py
[2025-01-06T21:02:21.323+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/scraping_1.py for tasks to queue
[2025-01-06T21:02:21.326+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:02:21.326+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/scraping_1.py
[2025-01-06T21:02:21.374+0000] {processor.py:840} INFO - DAG(s) 'filtered_links_workflow' retrieved from /opt/airflow/dags/scraping_1.py
[2025-01-06T21:02:21.541+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:02:21.541+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-01-06T21:02:21.557+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:02:21.557+0000] {dag.py:3823} INFO - Setting next_dagrun for filtered_links_workflow to None, run_after=None
[2025-01-06T21:02:21.584+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/scraping_1.py took 0.268 seconds
[2025-01-06T21:02:51.894+0000] {processor.py:161} INFO - Started process (PID=4156) to work on /opt/airflow/dags/scraping_1.py
[2025-01-06T21:02:51.896+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/scraping_1.py for tasks to queue
[2025-01-06T21:02:51.898+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:02:51.898+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/scraping_1.py
[2025-01-06T21:02:51.940+0000] {processor.py:840} INFO - DAG(s) 'filtered_links_workflow' retrieved from /opt/airflow/dags/scraping_1.py
[2025-01-06T21:02:52.129+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:02:52.129+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-01-06T21:02:52.149+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:02:52.148+0000] {dag.py:3823} INFO - Setting next_dagrun for filtered_links_workflow to None, run_after=None
[2025-01-06T21:02:52.377+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/scraping_1.py took 0.488 seconds
[2025-01-06T21:03:22.926+0000] {processor.py:161} INFO - Started process (PID=4169) to work on /opt/airflow/dags/scraping_1.py
[2025-01-06T21:03:22.928+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/scraping_1.py for tasks to queue
[2025-01-06T21:03:22.930+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:03:22.929+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/scraping_1.py
[2025-01-06T21:03:22.980+0000] {processor.py:840} INFO - DAG(s) 'filtered_links_workflow' retrieved from /opt/airflow/dags/scraping_1.py
[2025-01-06T21:03:23.142+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:03:23.141+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-01-06T21:03:23.370+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:03:23.369+0000] {dag.py:3823} INFO - Setting next_dagrun for filtered_links_workflow to None, run_after=None
[2025-01-06T21:03:23.407+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/scraping_1.py took 0.489 seconds
[2025-01-06T21:03:53.951+0000] {processor.py:161} INFO - Started process (PID=4181) to work on /opt/airflow/dags/scraping_1.py
[2025-01-06T21:03:53.954+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/scraping_1.py for tasks to queue
[2025-01-06T21:03:53.956+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:03:53.955+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/scraping_1.py
[2025-01-06T21:03:54.012+0000] {processor.py:840} INFO - DAG(s) 'filtered_links_workflow' retrieved from /opt/airflow/dags/scraping_1.py
[2025-01-06T21:03:54.216+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:03:54.216+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-01-06T21:03:54.234+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:03:54.233+0000] {dag.py:3823} INFO - Setting next_dagrun for filtered_links_workflow to None, run_after=None
[2025-01-06T21:03:54.267+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/scraping_1.py took 0.324 seconds
[2025-01-06T21:04:24.760+0000] {processor.py:161} INFO - Started process (PID=4185) to work on /opt/airflow/dags/scraping_1.py
[2025-01-06T21:04:24.762+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/scraping_1.py for tasks to queue
[2025-01-06T21:04:24.763+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:04:24.762+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/scraping_1.py
[2025-01-06T21:04:24.815+0000] {processor.py:840} INFO - DAG(s) 'filtered_links_workflow' retrieved from /opt/airflow/dags/scraping_1.py
[2025-01-06T21:04:25.021+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:04:25.021+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-01-06T21:04:25.041+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:04:25.040+0000] {dag.py:3823} INFO - Setting next_dagrun for filtered_links_workflow to None, run_after=None
[2025-01-06T21:04:25.072+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/scraping_1.py took 0.318 seconds
[2025-01-06T21:04:55.516+0000] {processor.py:161} INFO - Started process (PID=4197) to work on /opt/airflow/dags/scraping_1.py
[2025-01-06T21:04:55.518+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/scraping_1.py for tasks to queue
[2025-01-06T21:04:55.519+0000] {logging_mixin.py:188} INFO - [2025-01-06T21:04:55.519+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/scraping_1.py
[2025-01-06T21:04:55.565+0000] {processor.py:840} INFO - DAG(s) 'filtered_links_workflow' retrieved from /opt/airflow/dags/scraping_1.py
[2025-01-06T21:04:56.682+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (192.168.48.3), port 5432 failed: FATAL:  could not open file "global/pg_filenode.map": No such file or directory


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (192.168.48.3), port 5432 failed: FATAL:  could not open file "global/pg_filenode.map": No such file or directory

(Background on this error at: https://sqlalche.me/e/14/e3q8)
